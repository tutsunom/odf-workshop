= ODF Operatorを使用したストレージのデプロイ
include::_attributes.adoc[]
:profile: acs

== OCPクラスタのスケールと新しいWorkerノードの追加

このセクションでは、ODFリソースのために3つのWorkerノードを追加してOCPクラスタをスケールする前に、まずOCP環境のWorkerノードを確認します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get nodes -l node-role.kubernetes.io/worker -l '!node-role.kubernetes.io/infra','!node-role.kubernetes.io/master'
----
.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-129-208.us-east-2.compute.internal   Ready    worker   15m   v1.22.8+c02bd9d
ip-10-0-157-116.us-east-2.compute.internal   Ready    worker   15m   v1.22.8+c02bd9d
ip-10-0-185-31.us-east-2.compute.internal    Ready    worker   15m   v1.22.8+c02bd9d
ip-10-0-215-4.us-east-2.compute.internal     Ready    worker   15m   v1.22.8+c02bd9d
----

ここで *MachineSets* を使用して、さらに3つのWorkerノードをクラスタに追加することになります。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get machinesets -n openshift-machine-api | grep -v infra
----

ここでは、既にクラスタに存在するWorkerノードを作成するために使用される、既存の *MachineSets* が表示されます。3つのAWS Availability Zone (AZ)それぞれに *MachineSet* が存在することが分かります。

.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                    DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-d6qlm-mbttv-worker-us-east-2a   2         2         2       2           27m
cluster-d6qlm-mbttv-worker-us-east-2b   1         1         1       1           27m
cluster-d6qlm-mbttv-worker-us-east-2c   1         1         1       1           27m
----

インフラストラクチャーノードの演習と同様に、新しい *MachineSets* を作成して、各AWS AZにOCP クラスタのストレージ専用ノードを作成します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 3 workerocs 0 | oc create -f -
oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=workerocs -o name | xargs oc patch -n openshift-machine-api --type='json' -p '[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "role":"storage-node", "cluster.ocs.openshift.io/openshift-storage":""} }]'
oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=workerocs -o name | xargs oc scale -n openshift-machine-api --replicas=1
----

新しい *Machines* が作成されていることを確認します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get machines -n openshift-machine-api | egrep 'NAME|workerocs'
----

しばらくは `Provisioning` または `Provisioned` の状態にありますが、最終的には `Running` の状態になります。

.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                             PHASE     TYPE         REGION      ZONE         AGE
cluster-d6qlm-mbttv-workerocs-us-east-2a-v8p5k   Running   m5.4xlarge   us-east-2   us-east-2a   4m39s
cluster-d6qlm-mbttv-workerocs-us-east-2b-dx69b   Running   m5.4xlarge   us-east-2   us-east-2b   4m38s
cluster-d6qlm-mbttv-workerocs-us-east-2c-g7hlh   Running   m5.4xlarge   us-east-2   us-east-2c   4m38s
----

workerocs *Machines* はAWS EC2の `m5.4xlarge` インスタンスタイプを使用していることがわかります。

NOTE: `m5.4xlarge` インスタンスタイプは、16vCPUと64GBのメモリを持ち、ODFで推奨されるスペックです。 +

さて、私たちの新しい *Machines* がOCPクラスタに追加されているかどうかを確認します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"
----

新しい workerocs *Machine Set* の全てで `READY` と `AVAILABLE` のカラムに数値(この場合は `1` )が表示されるまで待ちます。このステップには5分以上かかる場合があります。

.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                       DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-d6qlm-mbttv-workerocs-us-east-2a   1         1         1       1           5m25s
cluster-d6qlm-mbttv-workerocs-us-east-2b   1         1         1       1           5m25s
cluster-d6qlm-mbttv-workerocs-us-east-2c   1         1         1       1           5m25s
----
kbd:[Ctrl+C]を押すと終了できます。

最後に、3つのWorkerノードが追加されていることを確認します。全てのworker nodeの `STATUS` が `Ready` であることを確認します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get nodes -l node-role.kubernetes.io/worker -l '!node-role.kubernetes.io/infra','!node-role.kubernetes.io/master'
----
.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-129-208.us-east-2.compute.internal   Ready    worker   45m   v1.22.8+c02bd9d
ip-10-0-142-148.us-east-2.compute.internal   Ready    worker   13m   v1.22.8+c02bd9d
ip-10-0-157-116.us-east-2.compute.internal   Ready    worker   45m   v1.22.8+c02bd9d
ip-10-0-173-143.us-east-2.compute.internal   Ready    worker   13m   v1.22.8+c02bd9d
ip-10-0-185-31.us-east-2.compute.internal    Ready    worker   45m   v1.22.8+c02bd9d
ip-10-0-215-4.us-east-2.compute.internal     Ready    worker   45m   v1.22.8+c02bd9d
ip-10-0-219-73.us-east-2.compute.internal    Ready    worker   13m   v1.22.8+c02bd9d
----

新しいOCP WorkerノードがODF用のラベルを持っていることを確認します。 +
ODFを稼働させるノードには、`cluster.ocs.openshift.io/openshift-storage` という特定のラベルが付いている必要があります。 +
先に `workerocs` *MachineSets* を作成した時にこのラベルを追加しています。これらの *MachineSets* を使って作成されたすべての *Machine* はこのラベルを持つことになります。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
----
.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-142-148.us-east-2.compute.internal   Ready    worker   14m   v1.22.8+c02bd9d
ip-10-0-173-143.us-east-2.compute.internal   Ready    worker   14m   v1.22.8+c02bd9d
ip-10-0-219-73.us-east-2.compute.internal    Ready    worker   14m   v1.22.8+c02bd9d
----

== ODF Operatorを使ったODFクラスタの作成

このセクションではOpenShift Data Foundation(ODF) Operatorをインストールし、新しく追加した3つのWorkerノードを使ってODFクラスタを作成します。 +
以下がインストールされます。
- ODF *OperatorGroup*
- ODF *Subscription*
- 他の全てのODF リソース (Operators, Ceph Pods, NooBaa Pods, StorageClasses)

はじめに `openshift-storage` Namespace を作成します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc create namespace openshift-storage
----

このNamespaceには、モニタリング用のラベルを追加する必要があります。これは、OCPストレージダッシュボードの Prometheus メトリクスとアラートを取得するために必要です。 +
`openshift-storage` Namespaceにラベルを付けるには、次のコマンドを使用します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"
----

NOTE: `openshift-storage` Namespaceの作成とモニタリング用のラベル付けは、*OpenShift Web Console* を使用してODF Operatorのインストール時に行うこともできます。


ログインしたら左側のメニューから、*Operators* -> *OperatorHub* を選択します。

.OCP OperatorHub
image::ocs/OCS-OCP-OperatorHub.png[OCP OperatorHub]

Now type `openshift data foundation` in the *Filter by _keyword..._* box.
*Filter by _keyword..._* のボックスに、`openshift data foundation` と入力します。

.OCP OperatorHub filter on OpenShift Data Foundation Operator
image::ocs/OCS4-OCP-OperatorHub-Filter.png[OCP OperatorHub Filter]

表示された `OpenShift Data Foundation Operator` を選択し、 *Install* ボタンを押します。

.OCP OperatorHub Install OpenShift Data Foundation
image::ocs/OCS4-OCP4-OperatorHub-Install.png[OCP OperatorHub Install]

次の画面で、設定が下図に示す通りであることを確認します。

.OCP Subscribe to OpenShift Data Foundation
image::ocs/OCS4-OCP4-OperatorHub-Subscribe.png[OCP OperatorHub Subscribe]

*Install* をクリックします。

ターミナルに戻って、下のコマンドを実行してインストール状況を確認できます。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
watch oc -n openshift-storage get csv
----
.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                  DISPLAY                       VERSION   REPLACES              PHASE
mcg-operator.v4.9.8   NooBaa Operator               4.9.8     mcg-operator.v4.9.7   Succeeded
ocs-operator.v4.9.8   OpenShift Container Storage   4.9.8     ocs-operator.v4.9.7   Succeeded
odf-operator.v4.9.8   OpenShift Data Foundation     4.9.8     odf-operator.v4.9.7   Succeeded
----
kbd:[Ctrl+C]を押すと終了できます。

リソース `csv` は `clusterserviceversions.operators.coreos.com` の短縮です。

.全てのOperatorの `PHASE` が `Succeeded` に変わるまで待って下さい。
CAUTION: 変わるまで数分かかる場合があります。

ODF Operatorのインストールが終わると、いくつかの新しいPodが `openshift-storage` Namespaceに作成されていることが確認できます。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc -n openshift-storage get pods
----
.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                               READY   STATUS    RESTARTS   AGE
noobaa-operator-75847d5b48-krtpt                   1/1     Running   0          5m37s
ocs-metrics-exporter-7f855fc64c-xlc7s              1/1     Running   0          5m35s
ocs-operator-7cdd8cc9f5-khwlg                      1/1     Running   0          5m35s
odf-console-6bb644f8c4-vndfh                       1/1     Running   0          5m49s
odf-operator-controller-manager-5b767b7f4c-6jm2j   2/2     Running   0          5m49s
rook-ceph-operator-54d974474c-k82xz                1/1     Running   0          5m35s
----

*Openshift Web Console* に戻ってそれでは続いてストレージクラスタを作成します。 +

*Create StorageSystem* をクリックします。

.Create storage system in openshift-storage namespace
image::ocs/OCS4-OCP4-View-Operator.png[Create storage system in openshift-storage namespace]

`Create StorageSystem` の画面が表示されます。

.Configure storage system settings
image::ocs/OCS4-config-screen-partial1.png[Configure storage system settings]

*Backing storage* では `Use an existing StorageClass` を選択し、*Storage Class* には `gp2` を指定します。 +
*Deployment type* では `Full deployment` を指定します。

*Next* をクリックします 。

NOTE: 他のメニューの `Create a new StorageClass using local storage devices` は、Baremetal方式でインストールしたOCPクラスタでODFを構成する場合や、AWS EBSではないEC2 Instanceに元から存在するデバイスを使ってODFクラスタを構成する場合に使います。 +
また `Connect an external storage platform` は、外部ストレージとコントロールプレーンを統一する特殊なケースで使います。

.Select capacity and nodes for new storage system
image::ocs/OCS4-config-screen-partial2.png[Select capacity and nodes for new storage system]

*Select Capacity* では、`2 TiB` を指定します。

CAUTION: *ここで選択する Requested Capacity は、将来容量を拡張する際の最小単位として利用されます。* +
例えば初めに2 TiBを選択した場合は、以降は 2TiB 単位で拡張することになります。

*Select nodes* で、ODFクラスタで使うnodeを指定して *Next* をクリックします。
ODF用のラベル `cluster.ocs.openshift.io/openshift-storage` が付けられたノードは、ここで自動で選択されるようになっています。そのため、はじめから3つのWorkerノードが選択されているはずです。以下のコマンドを実行して、間違いがないことを確認してみましょう。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get nodes --show-labels | grep ocs | cut -d ' ' -f1
----

*Next* をクリックします 。

.ODF create a new storage cluster: Security and network
image::ocs/ODF4.9-config-screen-partial3.png[Select encryption and network]

*Encryption* では、何も選択しません。 +
クラスタ全体、または部分的な暗号化を利用したい場合は、ここでチェックを入れます。今回の Lab では暗号化はしないので、チェックを外したままで構いません。 + 
（興味のある方は、チェックしてみてどのようなメニューが表示されるか確認されて構いません。*最後はチェックを外すよう注意してください*)

*Network* では、`Default (SDN)` を選択します。 +
Multus CNIを使ってPodで複数のネットワークを使用できる構成になっているOpenShiftクラスタでは、ODFでPublic NetworkとCluster Networkを分離することが可能です。 +
ここでは一般的な構成である、ネットワークを分離しないODFクラスタを構成するため、`Default (SDN)` を選択します。

*Next* をクリックします。

.Review and create new storage system
image::ocs/OCS4-config-screen-partial3.png[Review and create new storage system]

設定した内容をレビューし、問題がなければ *Create StorageSystem* をクリックします。

ターミナルウィンドウにすべての *Pods* が `Running` または `Completed` と表示されるまでお待ちください。これは5-10分かかります。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
watch oc -n openshift-storage get pods
----
.出力例
----
NAME                                                              READY   STATUS      RESTARTS   AGE
csi-cephfsplugin-5gmvm                                            3/3     Running     0          7m29s
csi-cephfsplugin-8z6tf                                            3/3     Running     0          7m29s
csi-cephfsplugin-ksznb                                            3/3     Running     0          7m29s
csi-cephfsplugin-lxjl4                                            3/3     Running     0          7m29s
csi-cephfsplugin-provisioner-b99bc4cbd-5r6lr                      6/6     Running     0          7m28s
csi-cephfsplugin-provisioner-b99bc4cbd-92fqb                      6/6     Running     0          7m28s
csi-cephfsplugin-r9kn7                                            3/3     Running     0          7m29s
csi-cephfsplugin-vv44h                                            3/3     Running     0          7m29s
csi-rbdplugin-4528q                                               3/3     Running     0          7m30s
csi-rbdplugin-8qgx2                                               3/3     Running     0          7m30s
csi-rbdplugin-9qrl5                                               3/3     Running     0          7m30s
csi-rbdplugin-dv6kr                                               3/3     Running     0          7m30s
csi-rbdplugin-f2lnk                                               3/3     Running     0          7m30s
csi-rbdplugin-provisioner-58dbf8596d-89nkc                        6/6     Running     0          7m30s
csi-rbdplugin-provisioner-58dbf8596d-crzkr                        6/6     Running     0          7m30s
csi-rbdplugin-z2hkc                                               3/3     Running     0          7m30s
noobaa-core-0                                                     1/1     Running     0          2m39s
noobaa-db-pg-0                                                    1/1     Running     0          2m40s
noobaa-endpoint-864c59cc59-p5bz5                                  1/1     Running     0          85s
noobaa-operator-585b66865d-z7n6d                                  1/1     Running     0          73m
ocs-metrics-exporter-6f7fb77856-hzqxm                             1/1     Running     0          73m
ocs-operator-f5bb58ddf-7ngr8                                      1/1     Running     0          73m
odf-console-87bb59fb4-f9mc2                                       1/1     Running     0          73m
odf-operator-controller-manager-85f6cbddfb-bnqd6                  2/2     Running     0          73m
rook-ceph-crashcollector-639901b7a01a84b64f7e5c4a655e8490-jbd9w   1/1     Running     0          3m58s
rook-ceph-crashcollector-8537d53bc818115d1313e67321d95993-bkhlg   1/1     Running     0          4m6s
rook-ceph-crashcollector-abe3991bcdfa5e2f397ccd4ef3879a78-gz87c   1/1     Running     0          4m5s
rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-cfb7488drfcjl   2/2     Running     0          3m2s
rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-7678d586nlzj7   2/2     Running     0          3m1s
rook-ceph-mgr-a-9b9d56-mk5r7                                      2/2     Running     0          4m6s
rook-ceph-mon-a-74965cfc9f-btt7c                                  2/2     Running     0          7m12s
rook-ceph-mon-b-987b79745-bg9nt                                   2/2     Running     0          4m59s
rook-ceph-mon-c-555d55585-9bf88                                   2/2     Running     0          4m35s
rook-ceph-operator-55b4496c6b-kmlm6                               1/1     Running     0          73m
rook-ceph-osd-0-74cb6855bd-l46rl                                  2/2     Running     0          3m34s
rook-ceph-osd-1-6b4f4cccdf-w7g7w                                  2/2     Running     0          3m33s
rook-ceph-osd-2-64c888b48-gcs4n                                   2/2     Running     0          3m26s
rook-ceph-osd-prepare-ocs-deviceset-gp2-0-data-0wnh5j--1-xhgmf    0/1     Completed   0          4m
rook-ceph-osd-prepare-ocs-deviceset-gp2-1-data-08kht7--1-zw66k    0/1     Completed   0          4m
rook-ceph-osd-prepare-ocs-deviceset-gp2-2-data-0blmfh--1-dzdqk    0/1     Completed   0          3m59s
----
kbd:[Ctrl+C]を押すと終了できます。

OperatorとOpenShiftの素晴らしいところは、デプロイされたコンポーネントに関するインテリジェンスをOperatorが内蔵していることです。
また、Operatorは `CustomResource` を定義します。そのため `CustomResource` 自体を見ることでステータスを確認することができます。 +
ODFを例にすると、ODFクラスタをデプロイすると最終的には `StorageSystem` と `StorageCluster` のインスタンスが生成されていることが分かります。この `StorageSystem` と `StorageCluster` は ODF Operator によって定義された `CustomeResource` です。

`StorageCluster` のステータスは次のようにチェックできます。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get storagecluster -n openshift-storage
----

`Phase` のカラムが `Ready` となっていれば、続けることができます。

### ストレージダッシュボードの使用

このセクションでは、*OpenShift Web Console* に含まれている、ODF独自のダッシュボードを使ってストレージクラスタのステータスを確認します。 +
まず、ODF Operatorのインストール後に画面右上に次のようなポップアップが表示されている場合は、*Refersh web console* をクリックして画面を更新してください。

.ODF Dashboard after successful operator installation
image::ocs/ODF4.9-refresh-webconsole.png[ODF Dashboard after successful operator installation]

ダッシュボードは左側のメニューバーから *Storage* -> *OpenShift Data Foundation* とクリックすることでアクセスできます。

NOTE: ODFのデプロイが完了したばかりの場合、ダッシュボードが完全に表示されるまでに5〜10分かかります。

.Storage Dashboard after successful storage installation
image::ocs/OCS-dashboard-healthy.png[Storage Dashboard after successful storage installation]

ダッシュボードの使用方法の詳細については、 https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/monitoring_openshift_data_foundation/cluster_health#verifying-openshift-data-foundation-is-healthy_rhodf[ODFモニタリングガイド]を参照してください。

全てが正常になると、ODFがインストール中に作成した3つの新しい *StorageClass* が使用可能になります。

- ocs-storagecluster-ceph-rbd
- ocs-storagecluster-cephfs
- openshift-storage.noobaa.io

*Storage* メニューの *Storage Classes* を選択することで、これら3つの *StorageClass* が表示されます。 +
また、以下のコマンドでも確認できます。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc get storageclasses
----

上記の3つの *StorageClass* が使用可能であることを確認しましょう。

NOTE: MCGは `noobaa-core` Pod内部の `db` コンテナで利用するために `ocs-storagecluster-ceph-rbd` StorageClassを使用してPVCを作成しています。

== Rook-Ceph toolboxを利用したCephクラスタの確認

このセクションでは、Rook-Ceph *toolbox* を利用して作成されたCephクラスタに対してcephコマンドを実行し、クラスタ構成を確認します。
Rook-Ceph *toolbox* はODFに同梱されていないため、手動でデプロイする必要があります。

以下のコマンドで `OCSInitialization ocsinit` を修正します。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch  '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

`rook-ceph-tools` Pod が `Running` になれば、次のようにtoolbox Podに入ることができます。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
----
*toolbox* Podに入ったら、次のcephコマンドを実行してみて下さい。これらのコマンドによってCephクラスタの詳細な構成を確認することができます。

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ceph status
----

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ceph osd status
----

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ceph osd tree
----

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ceph df
----

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
rados df
----

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
ceph versions
----

.出力例:
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
sh-4.4$ ceph status
  cluster:
    id:     cbeb7c9d-2a30-4646-b5a6-72d5c1db914c
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum a,b,c (age 19m)
    mgr: a(active, since 19m)
    mds: 1/1 daemons up, 1 hot standby
    osd: 3 osds: 3 up (since 18m), 3 in (since 19m)

  data:
    volumes: 1/1 healthy
    pools:   4 pools, 97 pgs
    objects: 92 objects, 133 MiB
    usage:   249 MiB used, 6.0 TiB / 6 TiB avail
    pgs:     97 active+clean

  io:
    client:   1.2 KiB/s rd, 5.0 KiB/s wr, 2 op/s rd, 0 op/s wr
----
kbd:[Ctrl+D] を押すか、 `exit` を実行して *toolbox* から出ることができます.


[.console-input]
[source,bash,subs="attributes+,+macros"]
----
exit
----
